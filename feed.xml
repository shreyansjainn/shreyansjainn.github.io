<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://shreyansjainn.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://shreyansjainn.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-05-07T11:14:02+00:00</updated><id>https://shreyansjainn.github.io/feed.xml</id><title type="html">Shreyans Jain</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Feature Geometry in Toy Models - Pentagon vs Hexagon</title><link href="https://shreyansjainn.github.io/blog/2025/pentagon-feature-geometry/" rel="alternate" type="text/html" title="Feature Geometry in Toy Models - Pentagon vs Hexagon"/><published>2025-01-28T02:40:16+00:00</published><updated>2025-01-28T02:40:16+00:00</updated><id>https://shreyansjainn.github.io/blog/2025/pentagon-feature-geometry</id><content type="html" xml:base="https://shreyansjainn.github.io/blog/2025/pentagon-feature-geometry/"><![CDATA[<h1 id="abstract">Abstract</h1> <p>For the past couple of years, toy models have been extensively studied in the field of mechanistic interpretability. The concept of superposition, feature geometry, and how correlated/anticorrelated/uncorrelated features arrange themselves in these situations was first studied in Toy Models itself. This is my attempt to explain the theoretical foundation on why feature geometry in the infinite data regime for a ReLU model (with hidden=2) caps at a Pentagon geometry irrespective of the number of inputs. Additionally, I analyze the deviations to hexagon geometry as well, primarily analyzing two factors, i.e., initialization and the choice of optimizer, and their impact on feature geometry.</p> <h1 id="introduction">Introduction</h1> <p>The ReLU output model is a toy model replication of a neural network showcasing the mapping of features (5 input features) to hidden dimensions (2 hidden dimensions) where superposition can be introduced by changing the sparsity of input features. For a set of input features $x∈R^n$ and a hidden layer vector $h∈R^m$, where $n»m$, the model is defined as follows:</p> \[h=Wx\] \[x^′=ReLU(W^{T}h+b)\] <p>This model showcases how a large set of input features can be represented in a much smaller dimensional vector in a state of <a href="https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=3br1psLRIjQCOv2T4RN3V6F2">superposition</a> by increasing the sparsity of the input features (sparsity tries to replicate the real-world data distribution where certain concepts are sparsely present throughout the training set). This concept and observations were first introduced in Anthropic’s <a href="https://transformer-circuits.pub/2022/toy_model/index.html">Toy Models of Superposition paper</a></p> <p>The original experiment was done on $n=5$ input features but when I try to increase the number of input features, an interesting phenomena occurs. Given the observations of a quadrilateral with $n=4$ and a pentagon with $n=5$, as the number of features increases one would expect feature geometry to be hexagon or heptagon, but in reality, the features restrict themselves to a pentagon geometry, with balance of the features (if they are learned by the model), overlapping with the existing 5 directions as shown in the figure below.</p> <p>I wanted to understand in detail why this is the case and if it can help us improve our understanding of feature geometry in neural networks. <br/></p> <p align="center"> <img src="/assets/img/pentagon_feature_geometry/n_6_superposition.png" width="200"/> <img src="/assets/img/pentagon_feature_geometry/n_7_superposition.png" width="200"/> <br/> </p> <p align="center"><b> Fig.1 n_features = 6 (left) and 7 (right) </b></p> <h1 id="relevance-to-ai-safety">Relevance to AI Safety</h1> <p>One of the biggest subfields of AI safety is Mechanistic Interpretability. In this subfield, researchers try to reverse-engineer neural network models (which are considered black boxes as their internal mechanisms are unknown) and make them more interpretable. Toy model setup is a very important part of this subfield, as it allows us to do controlled experiments and understand the impact of different factors on different phenomena.</p> <p>For example, the concept of superposition was first studied in Toy models of Superposition paper, which led to the creation of <a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html#setup-autoencoder-motivation">Sparse AutoEncoders (SAEs)</a>. Today, SAEs are an integral part of understanding the types of features a model is learning.</p> <p>By understanding feature geometries in toy models, we aim to develop a strong intuition about how and when features are arranged in different geometries and the types of mechanisms governing a given geometry.</p> <h1 id="different-data-regimes-and-types-of-features">Different Data Regimes and Types of Features</h1> <p>Before diving deep into feature geometry and the experiments, I would like to explain the concept of data regimes because that majorly impacts the type of features the model learns.</p> <p>The amount of data used to train the model has a massive impact on the types of features the model learns. <a href="https://transformer-circuits.pub/2023/toy-double-descent/index.html">Superposition, Memorization, and Double Descent</a> showcase the impact of data regimes and how the model memorizes the dataset as features in low data regimes but transitions to learning generalizable features as the amount of data increases.</p> <p><img src="/assets/img/pentagon_feature_geometry/double_descent_img.png" width="900" height="550" align="center"/></p> <p align="center"><b> Fig.2 Increase in the data size leads to features transitioning from data features to generalizing features and the geometry converges to a pentagon. Source: https://transformer-circuits.pub/2023/toy-double-descent/index.html </b></p> <p>I’ll be focusing on the infinite data regimes as it represents the closest condition to real-world models due to the sheer size of the datasets they are trained on. With the increase in data size, the feature geometry transitions towards the pentagon geometry, while in lower data regimes, the number of features represented was much higher due to the model memorizing the dataset.</p> <h1 id="theoretical-intuition-behind-pentagon-geometry">Theoretical Intuition behind Pentagon Geometry</h1> <p>For building the theoretical intuition for why pentagon geometry is preferred by models, I’ll be walking you through this <a href="https://colab.research.google.com/drive/1PTGgQt6OuWfAPi2iNn_myB4gQo-8ORAI?usp=sharing#scrollTo=WPyBYJMpxvyt">notebook</a> shared by Anthropic. (Credits to <a href="https://tomhenighan.com/">Tom Henighan</a> for open-sourcing this explanation)</p> <p>The key factors governing the feature geometry is the balance between the overall loss of ignored features and the loss of represented features, along with the number of features that are ignored vs represented.</p> <p>The expected loss due to ignoring a single feature can be defined as:</p> <p><img src="/assets/img/pentagon_feature_geometry/annotated_ignored_loss.png" width="900" height="450" align="center"/></p> <p align="center"><b> Fig.3 Expected loss due to ignoring a single feature </b></p> <p>Note: A sparse regime is consiered for this scenario, as only sparse features are represented in superposition and $S$ represents the probability of $x_i$ being 0 otherwise it takes a value uniformly sampled between 0 and 1.</p> <p>While the predicted value of $x_i’$ (when the feature is not ignored) can be defined by:</p> <p><img src="/assets/img/pentagon_feature_geometry/predicted_value_represented.png" width="900" height="350" align="center"/></p> <p align="center"><b> Fig.4 Predicted value when the feature is not ignored </b></p> <p>The formula represents the predicted value in the form of the original input value and then bakes in the influence of nearest neighbouring features (if it is positive interference or negative interference). $mod F$ term helps us wrap around the features around every $F$ positions and $(i \pm 1 \mod F)$ refer to the nearest neighbours represented feature index.</p> <p>After putting the above predicted term formula in the MSE loss formula, the loss due to the represented features can be defined by:</p> <p><img src="/assets/img/pentagon_feature_geometry/Represented_loss_annotated.png" width="900" height="350" align="center"/></p> <p align="center"><b> Fig.5 Loss due to the represented features </b></p> <p>Currently, there are two Loss terms, and based on number of features represented, these two loss terms will be pulling the overall loss in opposite directions. Loss due to represented features will increase with the increase in the number of represented features due to increased interference from neighbouring features, but we want to avoid the Loss due to ignored features as well. So, the final entity to be minimised is the difference between $L_F$ and $L_{\text{ignored}}$ i.e.</p> <p><img src="/assets/img/pentagon_feature_geometry/loss_difference_annotated.png" width="900" height="350" align="center"/></p> <p align="center"><b> Fig.6 Difference between $L_F$ and $L_{\text{ignored}}$ </b></p> <p>After optimizing for $\Delta L$, the optimal point of lowest loss is reached at $F=5$.</p> <p>The above derivation makes some assumptions for simplification and due to that the pentagon geometry is not always followed. There are some instances when a hexagon geometry (as shown in Fig 6) is observed as well. I’ll be conducting experiments to understand more about why this geometry was observed, what factors govern it and what is fundamentally different about hexagon geometry w.r.t pentagon geometry.</p> <p align="center"> <img src="/assets/img/pentagon_feature_geometry/hexagon_geometry.png" width="250"/> <br/> </p> <p align="center"><b> Fig.7 Hexagon Geometry in Toy models </b></p> <h1 id="experiments">Experiments</h1> <p>My experiment setup is similar to the Anthropic’s <a href="https://transformer-circuits.pub/2022/toy_model/index.html">Toy models of Superposition</a> setup with some modifications of my own. The number of input features will be 6 and all of them will have equal feature importance. I’ll be considering a sparsity value of 0.057 (as this sparsity was low enough to exhibit superposition but not low enough that features are not getting learned due to data unavailability) and will be analysing the effects of Initialization and the choice of Optimizer on hexagon feature geometry and superposition in general.</p> <h2 id="impact-of-initialization">Impact of Initialization</h2> <p>While iterating the experiments through multiple different seeds, there are multiple instances where the final geometry is a hexagon while others are a pentagon. Following are the lists of experiments I performed to understand this behaviour</p> <h3 id="feature-norms">Feature Norms</h3> <p>At the start, when I compare the feature norms, there is a striking difference in the first glance itself. The overall norm in the hexagon geometry crosses the theoretical limit of $\sqrt{5}$ and stretches to 3 while the one for pentagon plateaus at 2.5.</p> <p><img src="/assets/img/pentagon_feature_geometry/feature_norm_pentagon_vs_hexagon.png" width="900" height="200" align="center"/></p> <p align="center"><b> Fig.8 Feature Norms Pentagon vs Hexagon </b></p> <h3 id="per-feature-gradient-norms-and-losses">Per Feature Gradient Norms and Losses</h3> <p>As a next step, I analyzed the trajectory of the per-feature gradient norm (to understand the types of gradient updates taking place) and compared it with per-feature loss to learn how the features are being learned.</p> <p>In the case of pentagon geometry, the gradient updates for two features (dark blue and red) start at a very low point and stay low throughout the training run while for others it decreases in the later stages of the training process.</p> <p>When compared against the per-feature losses, I observe that one feature is starting from a point of very low loss (very near to the optimal point) so it doesn’t need big gradient updates, but the other one is starting from a very high loss (very far from the optimal point) and still has low gradient updates indicating that its stuck in a saddle point and is unable to escape.</p> <p><img src="/assets/img/pentagon_feature_geometry/pentagon_gradient_norm.png" width="900" height="230" align="center"/></p> <p align="center"><b> Fig.9 Per Feature Gradient Norm for Pentagon Seed </b></p> <p><img src="/assets/img/pentagon_feature_geometry/pentagon_per_feature_loss.png" width="900" height="230" align="center"/></p> <p align="center"><b> Fig.10 Per Feature Loss for Pentagon Seed </b></p> <p>In the similar plots for hexagon geometry, it shows that the gradient updates decreases for all the features and the losses for all the features converge to a optimal minima as well.</p> <p><img src="/assets/img/pentagon_feature_geometry/hexagon_gradient_norm.png" width="900" height="230" align="center"/></p> <p align="center"><b> Fig.11 Per Feature Gradient Norm for Hexagon Seed </b></p> <p><img src="/assets/img/pentagon_feature_geometry/hexagon_per_feature_loss.png" width="900" height="230" align="center"/></p> <p align="center"><b> Fig.12 Per Feature Loss for Hexagon Seed </b></p> <p>This analysis indicates the effect of initialization on the starting point of features in the coordinate space and how it affects the resulting feature geometry. To properly validate the hypothesis, I performed one more experiment of patching the initialization.</p> <h3 id="hexagon-patching">Hexagon Patching</h3> <p>I took the initial weights of the pentagon seed and replaced the weights of only the feature that is not learnt with initialization weights of the same feature from the hexagon seed. When the model is trained with this modified initial weight matrix, the resulting geometry showcases features arranged in a hexagon geometry after weight patching v/s in a pentagon geometry before patching.</p> <p align="center"> <img src="/assets/img/pentagon_feature_geometry/unpatched_pentagon.png" width="230" height="170"/> <img src="/assets/img/pentagon_feature_geometry/patched_hexagon.png" width="200"/> <br/> </p> <p align="center"><b> Fig.13 Unpatched Pentagon Geometry (left) and Patched Hexagon Geometry (right) </b></p> <h2 id="impact-of-optimizer">Impact of Optimizer</h2> <p>Secondly, I try to analyze the impact of the optimizer on the feature geometry. In this experiment, I chose 4 types of optimizers namely <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">SGD</a>, <a href="https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf?ref=ruder.io#page=26.00">RMSProp</a>, <a href="https://arxiv.org/abs/1412.6980">Adam</a>, and <a href="https://arxiv.org/abs/1711.05101">AdamW</a>.</p> <p>In the experiments, SGD underperformed in all cases leading to non-superimposed and unlearned features irrespective of hyperparameters. For RMSProp, Adam &amp; AdamW, all of them exhibit superposition and the resulting feature geometry was a delicate balance between initialization and choice of optimizer. Not all optimizers will give hexagon geometries at the same seed but once carefully chosen, the combination of the right seed and right optimizer will lead to the hexagon geometry, otherwise, it exhibits pentagon geometry.</p> <p align="center"> <img src="/assets/img/pentagon_feature_geometry/SGD.png" width="230" height="160"/> <img src="/assets/img/pentagon_feature_geometry/adam.png" width="200" height="160"/> <img src="/assets/img/pentagon_feature_geometry/adamW.png" width="200" height="160"/> <img src="/assets/img/pentagon_feature_geometry/rmsprop.png" width="200" height="160"/> <br/> </p> <p align="center"><b> Fig.14 SGD (first), Adam (Second), AdamW (Third), RMSProp (Fourth) </b></p> <h1 id="conclusion">Conclusion</h1> <p>In this analysis, I try to understand the core intuition behind the feature geometry in toy models with hidden dimension=2 and assumptions behind it. I also try to understand in what cases the experimental results deviate from the theoretical foundation leading to a hexagonal geometry and analyse the impact of Initialization and Optimizer on the resulting feature geometry.</p> <h1 id="limitations-and-future-work">Limitations and Future Work</h1> <p>While I found some pretty interesting results, there is still a much deeper analysis which can be conducted to study hexagon geometry and why it occurs. Also, this analysis was done on toy models so will the findings scale to bigger models or not, thats yet to be seen.</p> <h1 id="acknowledgement">Acknowledgement</h1> <p>I want to thank Bluedot Impact for organizing the AISF Alignment Cohort. Past 12 weeks have been really valuable in improving my understanding on Mechanistic Interpretability and AI Safety in general.</p> <p>Additionally, I want to thank <a href="https://www.linkedin.com/in/shivam-raval-27820484/">Shivam Raval</a> for guiding me through the research process.</p> <h1 id="about-me">About Me</h1> <p>Hey, I’m Shreyans and I’m an Independent Interpretability Researcher based out of Bengaluru, India. After 8 years of working in the Applied ML field, I finally took the leap of faith to take a small career break to pursue my interest of Interpretability Research. This work is my capstone project as part of the <a href="https://aisafetyfundamentals.com/alignment/">AISF Alignment</a> Cohort by Bluedot Impact. If you have any questions or feedback on this blog, feel free to reach out to me on jshrey8@gmail.com or any of my socials in my profile. I hope I was able to improve your understanding on feature geometry in toy models. Cheers!!</p>]]></content><author><name></name></author><category term="mech-interp,"/><category term="superposition"/><summary type="html"><![CDATA[Abstract]]></summary></entry><entry><title type="html">Effects of Non-Uniform Sparsity on Superposition in Toy Models</title><link href="https://shreyansjainn.github.io/blog/2024/non-uniform-sparsity/" rel="alternate" type="text/html" title="Effects of Non-Uniform Sparsity on Superposition in Toy Models"/><published>2024-11-19T02:40:16+00:00</published><updated>2024-11-19T02:40:16+00:00</updated><id>https://shreyansjainn.github.io/blog/2024/non-uniform-sparsity</id><content type="html" xml:base="https://shreyansjainn.github.io/blog/2024/non-uniform-sparsity/"><![CDATA[<p>Repost from LessWrong, check out th LW post <a href="https://www.lesswrong.com/posts/WwxG8RRHrorJgpoAk/effects-of-non-uniform-sparsity-on-superposition-in-toy">here</a></p> <h1 id="abstract">Abstract</h1> <p>This post summarises my findings on the effects of Non-Uniform feature sparsity on Superposition in the ReLU output model, introduced in the <a href="https://transformer-circuits.pub/2022/toy_model/index.html">Toy Models of Superposition paper</a>, the ReLU output model is a toy model which is shown to exhibit features in superposition instead of a dedicated dimension (‘individual neuron’) devoted to a single feature. That experiment showed how superposition is introduced in a model by varying the feature sparsity values. However, a uniform sparsity across all the features was considered to keep things interpretable and simple. This post explores the effects of non-uniform sparsity on superposition for a similar experiment setup.</p> <p>This question was listed in Neel Nanda’s <a href="https://www.lesswrong.com/posts/LbrPTJ4fmABEdEnLf/200-concrete-open-problems-in-mechanistic-interpretability">200 Concrete Open Problems in Mechanistic Interpretability</a> post. I’ve been interested in AI Interpretability for a long time but wasn’t sure how to enter the field. I discovered the field of mech-interp recently when I was working on some other project and instantly felt connected to the field. This is my first post on LW and this project is my attempt to increase my comfort working with mech-interp problems and build the necessary reasoning for solving more complex problems.</p> <h1 id="introduction">Introduction</h1> <p>The ReLU output model is a toy model replication of a neural network showcasing the mapping of features (5 input features) to hidden dimensions (2 hidden dimensions) where superposition can be introduced by changing the sparsity of input features. For a set of input features x∈Rn and a hidden layer vector h∈Rm, where n»m, the model is defined as follows:</p> \[h=Wx\] \[x^′=ReLU(W^{T}h+b)\] <p>This model showcases how a large set of input features can be represented in a much smaller dimensional vector in a state of superposition by increasing the sparsity of the input features (sparsity tries to replicate the real-world data distribution where certain concepts are sparsely present throughout the training set). This concept and observations were first introduced in Anthropic’s <a href="https://transformer-circuits.pub/2022/toy_model/index.html">Toy Models of Superposition paper</a></p> <p>Before we go ahead with the analysis, let me give you a primer on some important terms we will encounter in this post:</p> <ul> <li><strong>Feature Importance:</strong> Feature importance can be defined as how useful a particular feature is for achieving lower loss. It’s augmented in the ReLU model loss as a coefficient on the weighted mean squared error between the input and the output. $L=∑_x∑_iI_i(x_i−x^′_i)^2$ where $I_i$ is the importance of the feature $i$</li> <li><strong>Feature Sparsity:</strong> Feature sparsity is defined by how frequently a feature is present in the input data. In the ReLU Output model, its defined as the probability of the corresponding element in x being zero. An alternate quantity called <strong>Feature Probability</strong> defined as <strong>1 minus Sparsity</strong> is also used in this formulation.</li> </ul> <p>To summarise the paper’s findings, it showcases that as we increase the sparsity of input features, more features start getting represented in superposition. When sparsity is high, features of higher importance are represented as a dedicated dimension in the hidden layer, but as we increase the sparsity, lower-importance features start getting represented along with higher-importance features in superposition.</p> <p>In the below figures, as we move from left to right, feature sparsity is gradually increased showcasing the transition of how more and more features start getting represented in superposition.</p> <p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WwxG8RRHrorJgpoAk/x34emadicsfngl2fajcb" width="1100" height="150"/></p> <p>As we increase the feature sparsity from left to right, number of features represented by the hidden layer increases. Yellow represents the feature with highest importance and as we go more green, feature importance decreases</p> <p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WwxG8RRHrorJgpoAk/uhloldm237fdmdsgtay6" alt=""/></p> <p>Dark colors in the second figure mean feature is represented as a dedicated dimension and as the color transitions to yellow, it indicates feature being in superposition (Source: <a href="https://transformer-circuits.pub/2022/toy_model/index.html#demonstrating-basic-results">https://transformer-circuits.pub/2022/toy_model/index.html#demonstrating-basic-results</a>)</p> <p>I attempt to extend that representation one step further by considering a non-uniform sparsity instead of a uniform sparsity for input features as not all concepts are equally sparse in a training dataset.</p> <h1 id="experiment-setup">Experiment Setup</h1> <p>In my experiment setup, I’m considering two main levers, namely Feature Sparsity and Feature Importance. As we are considering non-uniform sparsity, its combined effect with feature importance plays an important role in the final result and leads to some interesting findings. To showcase different situations, I’m considering four different scenarios:</p> <ol> <li>Feature with the highest importance has the least sparsity and as we go down feature importance, the sparsity increases</li> <li>Feature with the highest importance has the highest sparsity and as we go down feature importance, the sparsity decreases</li> <li>Random feature sparsity across features</li> <li>Constant feature importance and increasing feature sparsity (similar to Case 1.)</li> </ol> <p>Also, to see the effect of random seed on the final results, every scenario was running eight instances while keeping everything constant.</p> <blockquote> <p><strong>Note:</strong> Similar figures that you will encounter from this point onwards has a slightly different interpretation. All the eight instances in every case have the same distribution of feature sparsity. They are to be interpreted as outputs of different random seeds while every other hyperparameter is kept constant. Feature sparsity is not decreasing as we move from left to right (unlike the preceding figures) in the following figures.</p> </blockquote> <h1 id="results">Results</h1> <h2 id="no-of-features-represented">No of Features Represented</h2> <p>In all the scenarios considered with non-uniform sparsity, never once are all the five input features represented in the hidden layer (as we saw in the case of uniform sparsity). No. of feature represented maxes out at 4 and hovers around 3 in a lot of scenarios. The below figure illustrates the phenomena for Case 1, for a detailed view of figures from all the scenarios, refer to this <a href="https://docs.google.com/document/d/1msBb3iH7JZlUgBkO2P_lF2-byEuqptJPr2zz0mm-LLA/edit?usp=sharing">page</a>.</p> <p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WwxG8RRHrorJgpoAk/dtwjvp3rbii0q1oot04w" width="1100" height="150"/></p> <p>Case 1: Feature with the highest importance (in yellow) has the least sparsity and as we go down feature importance (color gets greener), the sparsity increases</p> <p>Does this mean that in the case of non-uniform sparsity, all the features will never be represented in the hidden layer and we will always lose some concepts?</p> <p>To validate this from a different direction, I tried analyzing the number of features represented in a higher hidden dimension (h = 20, no of input features = 80). To estimate the amount of features represented in the hidden layer, I’m choosing the Frobenius Norm as a proxy. The table below summarises the Frobenium Norm values for all the scenarios considered:</p> <p>We can see, that the Frobenium Norm value in the case of Uniform Sparsity is the highest when compared to all the cases of Non-Uniform Sparsity which indicates the number of features represented when sparsity is uniform will most likely always be higher than when compared to non-uniform sparsity.</p> <table> <thead> <tr> <th style="text-align: center">Scenario</th> <th style="text-align: center">Frob Norm</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">Uniform Sparsity (Instance of lowest sparsity)</td> <td style="text-align: center">7.55</td> </tr> <tr> <td style="text-align: center">Case 1 (Max of all the instances)</td> <td style="text-align: center">4.98</td> </tr> <tr> <td style="text-align: center">Case 2 (Max of all the instances)</td> <td style="text-align: center">6.6</td> </tr> <tr> <td style="text-align: center">Case 3 (Max of all the instances)</td> <td style="text-align: center">5.75</td> </tr> <tr> <td style="text-align: center">Case 4 (Max of all the instances)</td> <td style="text-align: center">6.43</td> </tr> </tbody> </table> <p><br/></p> <h2 id="effects-on-superposition">Effects on Superposition</h2> <p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WwxG8RRHrorJgpoAk/iog1knqs63a6sglbvr32" alt=""/></p> <p>Superposition comparison between different cases considered. Purple color indicates features in dedicated dimension, green indicates feature in superposition with less interference, yellow indicates features in superposition with more interference. Blank indicates features not represented in hidden layer.</p> <p>In Case 1, we see the lowest feature representation and the least amount of superposition (showcased in yellow) which was an expected outcome as we are assigning the least amount of sparsity to the most important features. The blanks in the below figure indicate a particular feature is not represented at all in the hidden layer.</p> <p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WwxG8RRHrorJgpoAk/cwgl3q0y59ua2h77ojan" width="1200" height="450"/></p> <p>Case 1: showcasing more important features getting dedicated dimensions (in purple), while lower importance features are in superposition (in yellow). Blanks indicates features not represented in the hidden layer.</p> <p>Case 2 was pretty interesting as we noticed that all the features represented are always in superposition and none of the features is ever represented as a dedicated dimension irrespective of feature importance.</p> <p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WwxG8RRHrorJgpoAk/f3nb3zcfitybpm8lhgkm" width="1200" height="450"/></p> <p>Case 2: showcasing all the features (in yellow) in superpositon. Blanks indicates features not represented in the hidden layer.</p> <p>Case 3 is the closest we come to a real-world scenario, where feature sparsity is distributed randomly across features, irrespective of their importance. It looks like an averaged-out scenario of Cases 1 &amp; 2 and strikes an approx. midpoint between dedicated dimensions vs superposition when representing features.</p> <p>What exact combination of feature sparsity and feature importance governs this behaviour is something I wasn’t able to answer in my analysis and I think it’ll be an important question to answer as it can help us interpret the feature representations in real-world models.</p> <p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WwxG8RRHrorJgpoAk/hd726erhv7bvkonjxpzu" width="1200" height="450"/> Case 3: Some features are represented as dedicated dimension (in purple) and some are in superposition (in yellow)</p> <p>To check a clear effect of sparsity on superposition, we consider Case 4 where we keep the feature importance constant and consider an increasing sparsity (similar to Case 1.)<a href="https://www.lesswrong.com/posts/WwxG8RRHrorJgpoAk/effects-of-non-uniform-sparsity-on-superposition-in-toy#fngbx9ncdu8cc">[1]</a></p> <p>In this case, we stumble on a weird observation where the few features with the least sparsity are not even learned and represented in the hidden layer. The representation starts after a couple of features where all of the represented features (barring a few exceptions due to random seed) are in superposition all the time.</p> <p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WwxG8RRHrorJgpoAk/f9vquqtgfuklzbryv5lb" width="1200" height="450"/></p> <p>Case 4: Constant feature importance with increasing sparsity (as we go from top to bottom). Darker color (Green or dark blue) indicates less interference from other features meaning the feature is somewhat in superposition but to a lesser extent compared to features in yellow.</p> <p>Presently, I don’t have a clear intuition on why this is the case but this is quite a contrary observation compared to my expectation.</p> <p>Additionally, this scenario also gives us an intuition that feature importance has a bigger contribution in deciding whether a feature is represented as a dedicated dimension or not when compared to sparsity.</p>]]></content><author><name></name></author><category term="mech-interp,"/><category term="superposition"/><summary type="html"><![CDATA[Repost from LessWrong, check out th LW post here]]></summary></entry><entry><title type="html">Myth of ‘Are you Doing Enough?’</title><link href="https://shreyansjainn.github.io/blog/2024/are-you-doing-enough/" rel="alternate" type="text/html" title="Myth of ‘Are you Doing Enough?’"/><published>2024-10-08T12:40:16+00:00</published><updated>2024-10-08T12:40:16+00:00</updated><id>https://shreyansjainn.github.io/blog/2024/are-you-doing-enough</id><content type="html" xml:base="https://shreyansjainn.github.io/blog/2024/are-you-doing-enough/"><![CDATA[<p>In the past 8 years, my career trajectory has been a roller coaster of unreal magnitude. People looking from the outside might feel like I’ve made some insane progress, and my life is pretty sorted with decent savings, good work, and a stable (relatively) personal life. But during all this, my experience from my POV has been a different picture altogether. There has been one nagging question bothering me all the time: <strong>“ARE YOU DOING ENOUGH?”</strong> because to be honest, it never felt like I was.</p> <p>Just to give you some background, I’m a civil engineering graduate without any foundational CS, math or statistics knowledge and have been working as an Applied Data Scientist for the past 8 years. As of 2024, I’ve been interested in AI safety and ethics research for the past 3 years and really want to crack into it. So in a nutshell, for all my professional life, I’ve been playing catchup with whatever I’ve wanted to do. Initially, it was with ML fundamentals and trying to learn as much variety as possible, be it forecasting, recommender systems, search, deep learning, reinforcement learning, <strong>YOU JUST NAME IT!!</strong> If it involved math and CS somehow, I would be interested in that. Never once, did I felt like sitting comfortably wherever I was and just enjoy life.</p> <p>So an obvious by-product of all this juggling was a constant comparison with other folks present in the field and obviously (me being me!! 😕), the comparison was with accomplished folks in the field who have been working in that niche for multiple years (in other words, <em>WAY MORE TIME THAN ME!!</em>). I always had this urge to be able to learn all the tricks of that trade immediately and just master that niche.</p> <p>During all the comparative analysis I did to determine how I could reduce the gap between me and others (while obviously ignoring the obvious ones 🤪), I just put my foot on the gas pedal and was at it studying or working on projects, but at the end of the day, it never felt like I was doing enough to catch up, and whenever I compared again with others, I was always on a backfoot. This idea got ingrained in me so much that I almost believed that their systems were optimized to unreal extents in terms of productivity, focus and yield and to chase that I literally gave up on my hobbies and all kinds of entertainment (read as “rest” or “break”) but that feeling never went away.</p> <p>My mind wasn’t far behind in playing games either, it always moved benchmarks and goalposts while doing any kind of comparison, just so that I could arrive at the conclusion that <strong>I’m not doing enough!!</strong></p> <p>I still struggle with this idea to this day and there are days I spiral like the bestest lattu you’ve ever seen in your life. But slowly I’m getting to terms with the realisation that there is a high probability if you are asking yourself this question, <strong>YOU ARE DOING ENOUGH!!!</strong> already. The fact you are introspecting and trying to better yourself indicates that you have already explored lots of ideas and you are on the right path.</p> <p>In the end, I would just recommend one thing, don’t give up on your hobbies and fun projects, they make you a complete and interesting human being, because on the work side <strong>YOU ARE ALREADY DOING ENOUGH!!</strong>.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[In the past 8 years, my career trajectory has been a roller coaster of unreal magnitude. People looking from the outside might feel like I’ve made some insane progress, and my life is pretty sorted with decent savings, good work, and a stable (relatively) personal life. But during all this, my experience from my POV has been a different picture altogether. There has been one nagging question bothering me all the time: “ARE YOU DOING ENOUGH?” because to be honest, it never felt like I was.]]></summary></entry></feed>
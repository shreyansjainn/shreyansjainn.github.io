<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://shreyansjainn.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://shreyansjainn.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-29T07:28:33+00:00</updated><id>https://shreyansjainn.github.io/feed.xml</id><title type="html">Shreyans Jain</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Toy Models prefer pentagon geometry over everything else, why?</title><link href="https://shreyansjainn.github.io/blog/2025/pentagon-feature-geometry/" rel="alternate" type="text/html" title="Toy Models prefer pentagon geometry over everything else, why?"/><published>2025-01-28T02:40:16+00:00</published><updated>2025-01-28T02:40:16+00:00</updated><id>https://shreyansjainn.github.io/blog/2025/pentagon-feature-geometry</id><content type="html" xml:base="https://shreyansjainn.github.io/blog/2025/pentagon-feature-geometry/"><![CDATA[<p>Repost from LessWrong, check out th LW post <a href="https://www.lesswrong.com/posts/WwxG8RRHrorJgpoAk/effects-of-non-uniform-sparsity-on-superposition-in-toy">here</a></p> <h1 id="abstract">Abstract</h1> <p>For the past couple of years, toy models have been extensively studied in the field of mechanistic interpretability. We studied the introduction of superposition, feature geometry, and how correlation/anticorrelation/uncorrelated features arrange themselves in either situation. This work is my attempt to empirically validate and analyze why feature geometry in the infinite data regime for a ReLU model (with hidden=2) caps at a Pentagon geometry irrespective of the number of inputs and contribute to a better understanding of feature geometry in neural networks.</p> <p>I analyze two factors, i.e., initialization and optimizer, on how they affect the feature geometry. The experimental results align with the theoretical explanation of why the no of features cap at a pentagon geometry minimises the overall loss.</p> <h1 id="introduction">Introduction</h1> <p>The ReLU output model is a toy model replication of a neural network showcasing the mapping of features (5 input features) to hidden dimensions (2 hidden dimensions) where superposition can be introduced by changing the sparsity of input features. For a set of input featuresÂ xâˆˆRnÂ and a hidden layer vectorÂ hâˆˆRm, whereÂ nÂ»m, the model is defined as follows:</p> \[h=Wx\] \[x^â€²=ReLU(W^{T}h+b)\] <p>This model showcases how a large set of input features can be represented in a much smaller dimensional vector in a state of superposition by increasing the sparsity of the input features (sparsity tries to replicate the real-world data distribution where certain concepts are sparsely present throughout the training set). This concept and observations were first introduced in Anthropicâ€™sÂ <a href="https://transformer-circuits.pub/2022/toy_model/index.html">Toy Models of Superposition paper</a></p> <p>The original experiment was done on $n=5$ input features but when we try to increase the no of input features to more than 5, an interesting phenomena takes place. Given our observations of a quadrilateral with $n=4$ and a pentagon geometry with $n=5$, we would expect feature geometry to be hexagon or heptagon as we increase the no of features, but in reality, the features restrict themselves to a pentagon geometry itself, with the balance of the features (if they are learned by the model), overlapping with the existing 5 directions as show in the figure below.</p> <p>I wanted to understand and analyze in detail why is this the case and if it can help us improve our understanding of feature geometry in neural networks. <br/></p> <p align="center"> <img src="../assets/img/pentagon_feature_geometry/n_6_superposition.png" width="200"/> <img src="../assets/img/pentagon_feature_geometry/n_7_superposition.png" width="200"/> <br/> <b> Fig.1 - n_features = 6 (left) and 7 (right) </b> </p> <h1 id="different-data-regimes-and-types-of-features">Different Data Regimes and Types of Features</h1> <p>Before we dive deep into feature geometry and our experiments, I would like to explain the concept of data regimes because that majorly impacts the type of features our model learns.</p> <p>The amount of data used to train the model has a massive impact on the types of features the model learns. <a href="https://transformer-circuits.pub/2023/toy-double-descent/index.html">Superposition, Memorization, and Double Descent</a> showcase the impact of data regimes and how the model memorizes the dataset as features in low data regimes but transitions to learning generalizable features as the amount of data increases.</p> <p><img src="../assets/img/pentagon_feature_geometry/double_descent_img.png" alt=""/> <b> Fig.2 - As we increase the data size, features transition from data features towards generalizing features and the geometry converges to a pentagon. Source: https://transformer-circuits.pub/2023/toy-double-descent/index.html </b></p> <p>Weâ€™ll be focusing on the infinite data regimes as that represents the closest condition to real-world models due to the sheer size of datasets they are trained on and as we can see, when we increase the data size, the feature geometry transitions towards the pentagon geometry we talked about, while earlier, the no of features represented were much higher in lower data regimes.</p> <h1 id="theoretical-intuition-behind-pentagon-geometry">Theoretical Intuition behind Pentagon Geometry</h1> <p>For building the theoretical intuition for why pentagon geometry is preferred by models, weâ€™ll be walking you through this <a href="https://colab.research.google.com/drive/1PTGgQt6OuWfAPi2iNn_myB4gQo-8ORAI?usp=sharing#scrollTo=WPyBYJMpxvyt">notebook</a> shared by Anthropic. (Credits to <a href="https://tomhenighan.com/">Tom Henighan</a> for open-sourcing this explanation)</p> <p>The key factors governing the feature geometry is the balance between the overall loss of ignored features and the loss of represented features, along with the no of features that are ignored vs represented.</p> <p>We can define the expected loss due to ignoring a single feature by:</p> <p><img src="../assets/img/pentagon_feature_geometry/annotated_ignored_loss.png" alt=""/></p> <p>Note: We are considering a sparse regime, as only sparse features are represented in superposition and $S$ represents the probability of $x_i$ being 0 otherwise it takes a value uniformly sampled between 0 and 1.</p> <p>While the predicted value of $x_iâ€™$ (when the feature is not ignored) can be defined by:</p> <p><img src="../assets/img/pentagon_feature_geometry/predicted_value_represented.png" alt=""/></p> <p>The formula represents the predicted value in the form of the original input value and then bakes in the influence of nearest neighbouring features (if it is positive interference or negative interference). $mod F$ term helps us wrap around the features around every $F$ positions and $(i \pm 1 \mod F)$ refer to the nearest neighbours represented feature index.</p> <p>After putting the above predicted term formula in the MSE loss formula, we get the below term representing the Loss due to the represented features. <img src="../assets/img/pentagon_feature_geometry/Represented_loss_annotated.png" alt=""/></p> <p>Currently, we have two Loss terms, and based on no of features represented, two loss terms will be pulling the overall loss in opposing directions. Loss due to represented features will increase when we increase the no of represented features due to increased interference from neighbouring features, but we want to avoid the Loss due to ignored features as well. So, the entity we want to minizmise is the difference between $L_F$ and $L_{\text{ignored}}$ i.e.</p> <p><img src="../assets/img/pentagon_feature_geometry/loss_difference_annotated.png" alt=""/></p> <p>When we optimize for $\Delta L$, we reach the optimal point of lowest loss at $F=5$.</p> <p>During the above derivation, we make some assumptions for simplification and due to that the pentagon geometry is not always followed. There are some instances when a hexagon geometry is observed as well as weâ€™ll see in detail in the next section.</p>]]></content><author><name></name></author><category term="mech-interp,"/><category term="superposition"/><summary type="html"><![CDATA[Repost from LessWrong, check out th LW post here]]></summary></entry><entry><title type="html">Effects of Non-Uniform Sparsity on Superposition in Toy Models</title><link href="https://shreyansjainn.github.io/blog/2024/non-uniform-sparsity/" rel="alternate" type="text/html" title="Effects of Non-Uniform Sparsity on Superposition in Toy Models"/><published>2024-11-19T02:40:16+00:00</published><updated>2024-11-19T02:40:16+00:00</updated><id>https://shreyansjainn.github.io/blog/2024/non-uniform-sparsity</id><content type="html" xml:base="https://shreyansjainn.github.io/blog/2024/non-uniform-sparsity/"><![CDATA[<p>Repost from LessWrong, check out th LW post <a href="https://www.lesswrong.com/posts/WwxG8RRHrorJgpoAk/effects-of-non-uniform-sparsity-on-superposition-in-toy">here</a></p> <h1 id="abstract">Abstract</h1> <p>This post summarises my findings on the effects of Non-Uniform feature sparsity on Superposition in the ReLU output model, introduced in theÂ <a href="https://transformer-circuits.pub/2022/toy_model/index.html">Toy Models of Superposition paper</a>, the ReLU output model is a toy model which is shown to exhibit features in superposition instead of a dedicated dimension (â€˜individual neuronâ€™) devoted to a single feature. That experiment showed how superposition is introduced in a model by varying the feature sparsity values. However, a uniform sparsity across all the features was considered to keep things interpretable and simple. This post explores the effects of non-uniform sparsity on superposition for a similar experiment setup.</p> <p>This question was listed in Neel Nandaâ€™sÂ <a href="https://www.lesswrong.com/posts/LbrPTJ4fmABEdEnLf/200-concrete-open-problems-in-mechanistic-interpretability">200 Concrete Open Problems in Mechanistic Interpretability</a>Â post. Iâ€™ve been interested in AI Interpretability for a long time but wasnâ€™t sure how to enter the field. I discovered the field of mech-interp recently when I was working on some other project and instantly felt connected to the field. This is my first post on LW and this project is my attempt to increase my comfort working with mech-interp problems and build the necessary reasoning for solving more complex problems.</p> <h1 id="introduction">Introduction</h1> <p>The ReLU output model is a toy model replication of a neural network showcasing the mapping of features (5 input features) to hidden dimensions (2 hidden dimensions) where superposition can be introduced by changing the sparsity of input features. For a set of input featuresÂ xâˆˆRnÂ and a hidden layer vectorÂ hâˆˆRm, whereÂ nÂ»m, the model is defined as follows:</p> \[h=Wx\] \[x^â€²=ReLU(W^{T}h+b)\] <p>This model showcases how a large set of input features can be represented in a much smaller dimensional vector in a state of superposition by increasing the sparsity of the input features (sparsity tries to replicate the real-world data distribution where certain concepts are sparsely present throughout the training set). This concept and observations were first introduced in Anthropicâ€™sÂ <a href="https://transformer-circuits.pub/2022/toy_model/index.html">Toy Models of Superposition paper</a></p> <p>Before we go ahead with the analysis, let me give you a primer on some important terms we will encounter in this post:</p> <ul> <li><strong>Feature Importance:</strong>Â Feature importance can be defined as how useful a particular feature is for achieving lower loss. Itâ€™s augmented in the ReLU model loss as a coefficient on the weighted mean squared error between the input and the output. $L=âˆ‘_xâˆ‘_iI_i(x_iâˆ’x^â€²_i)^2$Â whereÂ $I_i$Â is the importance of the featureÂ $i$</li> <li><strong>Feature Sparsity:</strong>Â Feature sparsity is defined by how frequently a feature is present in the input data. In the ReLU Output model, its defined as the probability of the corresponding element inÂ xÂ being zero. An alternate quantity calledÂ <strong>Feature Probability</strong>Â defined asÂ <strong>1 minus Sparsity</strong>Â is also used in this formulation.</li> </ul> <p>To summarise the paperâ€™s findings, it showcases that as we increase the sparsity of input features, more features start getting represented in superposition. When sparsity is high, features of higher importance are represented as a dedicated dimension in the hidden layer, but as we increase the sparsity, lower-importance features start getting represented along with higher-importance features in superposition.</p> <p>In the below figures, as we move from left to right, feature sparsity is gradually increased showcasing the transition of how more and more features start getting represented in superposition.</p> <p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WwxG8RRHrorJgpoAk/x34emadicsfngl2fajcb" width="1100" height="150"/></p> <p>As we increase the feature sparsity from left to right, number of features represented by the hidden layer increases. Yellow represents the feature with highest importance and as we go more green, feature importance decreases</p> <p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WwxG8RRHrorJgpoAk/uhloldm237fdmdsgtay6" alt=""/></p> <p>Dark colors in the second figure mean feature is represented as a dedicated dimension and as the color transitions to yellow, it indicates feature being in superposition (Source:Â <a href="https://transformer-circuits.pub/2022/toy_model/index.html#demonstrating-basic-results">https://transformer-circuits.pub/2022/toy_model/index.html#demonstrating-basic-results</a>)</p> <p>I attempt to extend that representation one step further by considering a non-uniform sparsity instead of a uniform sparsity for input features as not all concepts are equally sparse in a training dataset.</p> <h1 id="experiment-setup">Experiment Setup</h1> <p>In my experiment setup, Iâ€™m considering two main levers, namely Feature Sparsity and Feature Importance. As we are considering non-uniform sparsity, its combined effect with feature importance plays an important role in the final result and leads to some interesting findings. To showcase different situations, Iâ€™m considering four different scenarios:</p> <ol> <li>Feature with the highest importance has the least sparsity and as we go down feature importance, the sparsity increases</li> <li>Feature with the highest importance has the highest sparsity and as we go down feature importance, the sparsity decreases</li> <li>Random feature sparsity across features</li> <li>Constant feature importance and increasing feature sparsity (similar to Case 1.)</li> </ol> <p>Also, to see the effect of random seed on the final results, every scenario was running eight instances while keeping everything constant.</p> <blockquote> <p><strong>Note:</strong>Â Similar figures that you will encounter from this point onwards has a slightly different interpretation. All the eight instances in every case have the same distribution of feature sparsity. They are to be interpreted as outputs of different random seeds while every other hyperparameter is kept constant. Feature sparsity is not decreasing as we move from left to right (unlike the preceding figures) in the following figures.</p> </blockquote> <h1 id="results">Results</h1> <h2 id="no-of-features-represented">No of Features Represented</h2> <p>In all the scenarios considered with non-uniform sparsity, never once are all the five input features represented in the hidden layer (as we saw in the case of uniform sparsity). No. of feature represented maxes out at 4 and hovers around 3 in a lot of scenarios. The below figure illustrates the phenomena for Case 1, for a detailed view of figures from all the scenarios, refer to thisÂ <a href="https://docs.google.com/document/d/1msBb3iH7JZlUgBkO2P_lF2-byEuqptJPr2zz0mm-LLA/edit?usp=sharing">page</a>.</p> <p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WwxG8RRHrorJgpoAk/dtwjvp3rbii0q1oot04w" width="1100" height="150"/></p> <p>Case 1: Feature with the highest importance (in yellow) has the least sparsity and as we go down feature importance (color gets greener), the sparsity increases</p> <p>Does this mean that in the case of non-uniform sparsity, all the features will never be represented in the hidden layer and we will always lose some concepts?</p> <p>To validate this from a different direction, I tried analyzing the number of features represented in a higher hidden dimension (h = 20, no of input features = 80). To estimate the amount of features represented in the hidden layer, Iâ€™m choosing the Frobenius Norm as a proxy. The table below summarises the Frobenium Norm values for all the scenarios considered:</p> <p>We can see, that the Frobenium Norm value in the case of Uniform Sparsity is the highest when compared to all the cases of Non-Uniform Sparsity which indicates the number of features represented when sparsity is uniform will most likely always be higher than when compared to non-uniform sparsity.</p> <table> <thead> <tr> <th style="text-align: center">Scenario</th> <th style="text-align: center">Frob Norm</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">Uniform Sparsity (Instance of lowest sparsity)</td> <td style="text-align: center">7.55</td> </tr> <tr> <td style="text-align: center">Case 1 (Max of all the instances)</td> <td style="text-align: center">4.98</td> </tr> <tr> <td style="text-align: center">Case 2 (Max of all the instances)</td> <td style="text-align: center">6.6</td> </tr> <tr> <td style="text-align: center">Case 3 (Max of all the instances)</td> <td style="text-align: center">5.75</td> </tr> <tr> <td style="text-align: center">Case 4 (Max of all the instances)</td> <td style="text-align: center">6.43</td> </tr> </tbody> </table> <p><br/></p> <h2 id="effects-on-superposition">Effects on Superposition</h2> <p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WwxG8RRHrorJgpoAk/iog1knqs63a6sglbvr32" alt=""/></p> <p>Superposition comparison between different cases considered. Purple color indicates features in dedicated dimension, green indicates feature in superposition with less interference, yellow indicates features in superposition with more interference. Blank indicates features not represented in hidden layer.</p> <p>In Case 1, we see the lowest feature representation and the least amount of superposition (showcased in yellow) which was an expected outcome as we are assigning the least amount of sparsity to the most important features. The blanks in the below figure indicate a particular feature is not represented at all in the hidden layer.</p> <p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WwxG8RRHrorJgpoAk/cwgl3q0y59ua2h77ojan" width="1200" height="450"/></p> <p>Case 1: showcasing more important features getting dedicated dimensions (in purple), while lower importance features are in superposition (in yellow). Blanks indicates features not represented in the hidden layer.</p> <p>Case 2 was pretty interesting as we noticed that all the features represented are always in superposition and none of the features is ever represented as a dedicated dimension irrespective of feature importance.</p> <p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WwxG8RRHrorJgpoAk/f3nb3zcfitybpm8lhgkm" width="1200" height="450"/></p> <p>Case 2: showcasing all the features (in yellow) in superpositon. Blanks indicates features not represented in the hidden layer.</p> <p>Case 3 is the closest we come to a real-world scenario, where feature sparsity is distributed randomly across features, irrespective of their importance. It looks like an averaged-out scenario of Cases 1 &amp; 2 and strikes an approx. midpoint between dedicated dimensions vs superposition when representing features.</p> <p>What exact combination of feature sparsity and feature importance governs this behaviour is something I wasnâ€™t able to answer in my analysis and I think itâ€™ll be an important question to answer as it can help us interpret the feature representations in real-world models.</p> <p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WwxG8RRHrorJgpoAk/hd726erhv7bvkonjxpzu" width="1200" height="450"/> Case 3: Some features are represented as dedicated dimension (in purple) and some are in superposition (in yellow)</p> <p>To check a clear effect of sparsity on superposition, we consider Case 4 where we keep the feature importance constant and consider an increasing sparsity (similar to Case 1.)<a href="https://www.lesswrong.com/posts/WwxG8RRHrorJgpoAk/effects-of-non-uniform-sparsity-on-superposition-in-toy#fngbx9ncdu8cc">[1]</a></p> <p>In this case,Â we stumble on a weird observation where the few features with the least sparsity are not even learned and represented in the hidden layer. The representation starts after a couple of features where all of the represented features (barring a few exceptions due to random seed) are in superposition all the time.</p> <p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WwxG8RRHrorJgpoAk/f9vquqtgfuklzbryv5lb" width="1200" height="450"/></p> <p>Case 4: Constant feature importance with increasing sparsity (as we go from top to bottom). Darker color (Green or dark blue) indicates less interference from other features meaning the feature is somewhat in superposition but to a lesser extent compared to features in yellow.</p> <p>Presently, I donâ€™t have a clear intuition on why this is the case but this is quite a contrary observation compared to my expectation.</p> <p>Additionally, this scenario also gives us an intuition that feature importance has a bigger contribution in deciding whether a feature is represented as a dedicated dimension or not when compared to sparsity.</p>]]></content><author><name></name></author><category term="mech-interp,"/><category term="superposition"/><summary type="html"><![CDATA[Repost from LessWrong, check out th LW post here]]></summary></entry><entry><title type="html">Myth of â€˜Are you Doing Enough?â€™</title><link href="https://shreyansjainn.github.io/blog/2024/are-you-doing-enough/" rel="alternate" type="text/html" title="Myth of â€˜Are you Doing Enough?â€™"/><published>2024-10-08T12:40:16+00:00</published><updated>2024-10-08T12:40:16+00:00</updated><id>https://shreyansjainn.github.io/blog/2024/are-you-doing-enough</id><content type="html" xml:base="https://shreyansjainn.github.io/blog/2024/are-you-doing-enough/"><![CDATA[<p>In the past 8 years, my career trajectory has been a roller coaster of unreal magnitude. People looking from the outside might feel like Iâ€™ve made some insane progress, and my life is pretty sorted with decent savings, good work, and a stable (relatively) personal life. But during all this, my experience from my POV has been a different picture altogether. There has been one nagging question bothering me all the time: <strong>â€œARE YOU DOING ENOUGH?â€</strong> because to be honest, it never felt like I was.</p> <p>Just to give you some background, Iâ€™m a civil engineering graduate without any foundational CS, math or statistics knowledge and have been working as an Applied Data Scientist for the past 8 years. As of 2024, Iâ€™ve been interested in AI safety and ethics research for the past 3 years and really want to crack into it. So in a nutshell, for all my professional life, Iâ€™ve been playing catchup with whatever Iâ€™ve wanted to do. Initially, it was with ML fundamentals and trying to learn as much variety as possible, be it forecasting, recommender systems, search, deep learning, reinforcement learning, <strong>YOU JUST NAME IT!!</strong> If it involved math and CS somehow, I would be interested in that. Never once, did I felt like sitting comfortably wherever I was and just enjoy life.</p> <p>So an obvious by-product of all this juggling was a constant comparison with other folks present in the field and obviously (me being me!! ğŸ˜•), the comparison was with accomplished folks in the field who have been working in that niche for multiple years (in other words, <em>WAY MORE TIME THAN ME!!</em>). I always had this urge to be able to learn all the tricks of that trade immediately and just master that niche.</p> <p>During all the comparative analysis I did to determine how I could reduce the gap between me and others (while obviously ignoring the obvious ones ğŸ¤ª), I just put my foot on the gas pedal and was at it studying or working on projects, but at the end of the day, it never felt like I was doing enough to catch up, and whenever I compared again with others, I was always on a backfoot. This idea got ingrained in me so much that I almost believed that their systems were optimized to unreal extents in terms of productivity, focus and yield and to chase that I literally gave up on my hobbies and all kinds of entertainment (read as â€œrestâ€ or â€œbreakâ€) but that feeling never went away.</p> <p>My mind wasnâ€™t far behind in playing games either, it always moved benchmarks and goalposts while doing any kind of comparison, just so that I could arrive at the conclusion that <strong>Iâ€™m not doing enough!!</strong></p> <p>I still struggle with this idea to this day and there are days I spiral like the bestest lattu youâ€™ve ever seen in your life. But slowly Iâ€™m getting to terms with the realisation that there is a high probability if you are asking yourself this question, <strong>YOU ARE DOING ENOUGH!!!</strong> already. The fact you are introspecting and trying to better yourself indicates that you have already explored lots of ideas and you are on the right path.</p> <p>In the end, I would just recommend one thing, donâ€™t give up on your hobbies and fun projects, they make you a complete and interesting human being, because on the work side <strong>YOU ARE ALREADY DOING ENOUGH!!</strong>.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[In the past 8 years, my career trajectory has been a roller coaster of unreal magnitude. People looking from the outside might feel like Iâ€™ve made some insane progress, and my life is pretty sorted with decent savings, good work, and a stable (relatively) personal life. But during all this, my experience from my POV has been a different picture altogether. There has been one nagging question bothering me all the time: â€œARE YOU DOING ENOUGH?â€ because to be honest, it never felt like I was.]]></summary></entry></feed>
<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Toy Models prefer pentagon geometry over everything else, why? | Shreyans Jain </title> <meta name="author" content="Shreyans Jain"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://shreyansjainn.github.io/blog/2025/pentagon-feature-geometry/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}};</script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Shreyans Jain </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/favblog/">repeat reads </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Toy Models prefer pentagon geometry over everything else, why?</h1> <p class="post-meta"> Created in January 28, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/mech-interp"> <i class="fa-solid fa-hashtag fa-sm"></i> mech-interp,</a>   <a href="/blog/tag/superposition"> <i class="fa-solid fa-hashtag fa-sm"></i> superposition</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="abstract">Abstract</h1> <p>For the past couple of years, toy models have been extensively studied in the field of mechanistic interpretability. We studied the introduction of superposition, feature geometry, and how correlation/anticorrelation/uncorrelated features arrange themselves in either situation. This work is my attempt to empirically validate and analyze why feature geometry in the infinite data regime for a ReLU model (with hidden=2) caps at a Pentagon geometry irrespective of the number of inputs and contribute to a better understanding of feature geometry in neural networks.</p> <p>I analyze two factors, i.e., initialization and optimizer, on how they affect the feature geometry. The experimental results align with the theoretical explanation of why the no of features cap at a pentagon geometry minimises the overall loss.</p> <h1 id="introduction">Introduction</h1> <p>The ReLU output model is a toy model replication of a neural network showcasing the mapping of features (5 input features) to hidden dimensions (2 hidden dimensions) where superposition can be introduced by changing the sparsity of input features. For a set of input features x∈Rn and a hidden layer vector h∈Rm, where n»m, the model is defined as follows:</p> \[h=Wx\] \[x^′=ReLU(W^{T}h+b)\] <p>This model showcases how a large set of input features can be represented in a much smaller dimensional vector in a state of superposition by increasing the sparsity of the input features (sparsity tries to replicate the real-world data distribution where certain concepts are sparsely present throughout the training set). This concept and observations were first introduced in Anthropic’s <a href="https://transformer-circuits.pub/2022/toy_model/index.html" rel="external nofollow noopener" target="_blank">Toy Models of Superposition paper</a></p> <p>The original experiment was done on $n=5$ input features but when we try to increase the no of input features to more than 5, an interesting phenomena takes place. Given our observations of a quadrilateral with $n=4$ and a pentagon geometry with $n=5$, we would expect feature geometry to be hexagon or heptagon as we increase the no of features, but in reality, the features restrict themselves to a pentagon geometry itself, with the balance of the features (if they are learned by the model), overlapping with the existing 5 directions as show in the figure below.</p> <p>I wanted to understand and analyze in detail why is this the case and if it can help us improve our understanding of feature geometry in neural networks. <br></p> <p align="center"> <img src="./assets/img/pentagon_feature_geometry/n_6_superposition.png" width="200"> <img src="./assets/img/pentagon_feature_geometry/n_7_superposition.png" width="200"> <br> <b> Fig.1 - n_features = 6 (left) and 7 (right) </b> </p> <h1 id="different-data-regimes-and-types-of-features">Different Data Regimes and Types of Features</h1> <p>Before we dive deep into feature geometry and our experiments, I would like to explain the concept of data regimes because that majorly impacts the type of features our model learns.</p> <p>The amount of data used to train the model has a massive impact on the types of features the model learns. <a href="https://transformer-circuits.pub/2023/toy-double-descent/index.html" rel="external nofollow noopener" target="_blank">Superposition, Memorization, and Double Descent</a> showcase the impact of data regimes and how the model memorizes the dataset as features in low data regimes but transitions to learning generalizable features as the amount of data increases.</p> <p><img src="../assets/img/pentagon_feature_geometry/double_descent_img.png" alt=""> <b> Fig.2 - As we increase the data size, features transition from data features towards generalizing features and the geometry converges to a pentagon. Source: https://transformer-circuits.pub/2023/toy-double-descent/index.html </b></p> <p>We’ll be focusing on the infinite data regimes as that represents the closest condition to real-world models due to the sheer size of datasets they are trained on and as we can see, when we increase the data size, the feature geometry transitions towards the pentagon geometry we talked about, while earlier, the no of features represented were much higher in lower data regimes.</p> <h1 id="theoretical-intuition-behind-pentagon-geometry">Theoretical Intuition behind Pentagon Geometry</h1> <p>For building the theoretical intuition for why pentagon geometry is preferred by models, we’ll be walking you through this <a href="https://colab.research.google.com/drive/1PTGgQt6OuWfAPi2iNn_myB4gQo-8ORAI?usp=sharing#scrollTo=WPyBYJMpxvyt" rel="external nofollow noopener" target="_blank">notebook</a> shared by Anthropic. (Credits to <a href="https://tomhenighan.com/" rel="external nofollow noopener" target="_blank">Tom Henighan</a> for open-sourcing this explanation)</p> <p>The key factors governing the feature geometry is the balance between the overall loss of ignored features and the loss of represented features, along with the no of features that are ignored vs represented.</p> <p>We can define the expected loss due to ignoring a single feature by:</p> <p><img src="../assets/img/pentagon_feature_geometry/annotated_ignored_loss.png" alt=""></p> <p>Note: We are considering a sparse regime, as only sparse features are represented in superposition and $S$ represents the probability of $x_i$ being 0 otherwise it takes a value uniformly sampled between 0 and 1.</p> <p>While the predicted value of $x_i’$ (when the feature is not ignored) can be defined by:</p> <p><img src="../assets/img/pentagon_feature_geometry/predicted_value_represented.png" alt=""></p> <p>The formula represents the predicted value in the form of the original input value and then bakes in the influence of nearest neighbouring features (if it is positive interference or negative interference). $mod F$ term helps us wrap around the features around every $F$ positions and $(i \pm 1 \mod F)$ refer to the nearest neighbours represented feature index.</p> <p>After putting the above predicted term formula in the MSE loss formula, we get the below term representing the Loss due to the represented features. <img src="../assets/img/pentagon_feature_geometry/Represented_loss_annotated.png" alt=""></p> <p>Currently, we have two Loss terms, and based on no of features represented, two loss terms will be pulling the overall loss in opposing directions. Loss due to represented features will increase when we increase the no of represented features due to increased interference from neighbouring features, but we want to avoid the Loss due to ignored features as well. So, the entity we want to minizmise is the difference between $L_F$ and $L_{\text{ignored}}$ i.e.</p> <p><img src="../assets/img/pentagon_feature_geometry/loss_difference_annotated.png" alt=""></p> <p>When we optimize for $\Delta L$, we reach the optimal point of lowest loss at $F=5$.</p> <p>During the above derivation, we make some assumptions for simplification and due to that the pentagon geometry is not always followed. There are some instances when a hexagon geometry is observed as well as we’ll see in detail in the next section.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/non-uniform-sparsity/">Effects of Non-Uniform Sparsity on Superposition in Toy Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/are-you-doing-enough/">Myth of 'Are you Doing Enough?'</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Shreyans Jain. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"projects",description:"A growing collection of your cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-cv",title:"cv",description:"at a glance",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-repeat-reads",title:"repeat reads",description:"List of Blogs/articles i like to revisit from time to time",section:"Navigation",handler:()=>{window.location.href="/favblog/"}},{id:"post-toy-models-prefer-pentagon-geometry-over-everything-else-why",title:"Toy Models prefer pentagon geometry over everything else, why?",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/pentagon-feature-geometry/"}},{id:"post-effects-of-non-uniform-sparsity-on-superposition-in-toy-models",title:"Effects of Non-Uniform Sparsity on Superposition in Toy Models",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/non-uniform-sparsity/"}},{id:"post-myth-of-39-are-you-doing-enough-39",title:"Myth of 'Are you Doing Enough?'",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/are-you-doing-enough/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6A%73%68%72%65%79%38@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/shreyansjainn","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/shreyans-jain-4b063667","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/py_parrot","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>